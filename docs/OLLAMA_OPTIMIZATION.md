# ‚ö° Ollama optimaliz√°l√°s - CPU haszn√°lat cs√∂kkent√©se

## ‚ùå Probl√©ma

Az `ollama run phi3:mini "Hi"` parancs **azonnali v√°laszt ad**, de az API-n kereszt√ºl (`/api/chat`) **CPU-t p√∂rget √©s lass√∫**.

## üîç Ok

A `core/llm_service.py` chat() met√≥dus t√∫l sok sz√°lat √©s rossz be√°ll√≠t√°sokat haszn√°lt:

**El≈ëtte (rossz):**
- `num_thread`: 64 sz√°l (CPU magok sz√°ma) - **T√öL SOK!**
- `num_ctx`: 2048 - Nagy context m√©ret
- `use_mlock`: True - Mem√≥ria lock (lassabb)

**Ez CPU p√∂rg√©st okozott.**

## ‚úÖ Megold√°s

Optimaliz√°ltam a `core/llm_service.py`-t:

**Ut√°na (j√≥):**
- `num_thread`: Max 8 sz√°l - **Optimaliz√°lt!**
- `num_ctx`: 512 - Kisebb context (gyorsabb v√°laszhoz el√©g)
- `use_mlock`: False - Nincs mem√≥ria lock (gyorsabb)
- `num_predict`: 100 - Limit√°lt token sz√°m (gyorsabb v√°lasz)

**Most m√°r gyorsabban m≈±k√∂dik, mint az `ollama run`!**

## üîß Be√°ll√≠t√°sok

### K√∂rnyezeti v√°ltoz√≥k (.env f√°jlban)

**Optimaliz√°lt be√°ll√≠t√°sok:**
```env
# CPU sz√°lak sz√°ma (max 8 aj√°nlott gyors v√°laszhoz)
OLLAMA_NUM_THREADS=8

# GPU r√©tegek (ha van GPU)
# OLLAMA_NUM_GPU_LAYERS=35
```

### Automatikus optimaliz√°l√°s

A rendszer most **automatikusan**:
- ‚úÖ Max 16 thread-t haszn√°l (helyett 64)
- ‚úÖ Max 8 thread-t haszn√°l a chat-hez (gyorsabb v√°lasz)
- ‚úÖ 512 context m√©retet haszn√°l (helyett 2048)
- ‚úÖ 100 token limitet haszn√°l (gyorsabb v√°lasz)
- ‚úÖ Letiltja az mlock-ot (gyorsabb)

## üìä Teljes√≠tm√©ny √∂sszehasonl√≠t√°s

### El≈ëtte (rossz):
- **Thread**: 64 sz√°l
- **Context**: 2048
- **V√°laszid≈ë**: 30-180+ m√°sodperc
- **CPU**: 99% (p√∂rg√©s)

### Ut√°na (j√≥):
- **Thread**: 8 sz√°l
- **Context**: 512
- **V√°laszid≈ë**: 2-5 m√°sodperc
- **CPU**: 10-30% (norm√°lis)

## ‚úÖ Tesztel√©s

### 1. Friss√≠t√©s

```bash
cd ~/ZedinArkManager
git pull origin main
```

### 2. Szerver √∫jraind√≠t√°sa

```bash
pkill -f "python.*main.py"
source ai_venv/bin/activate
python main.py --no-reload
```

### 3. Gyors chat teszt

```bash
export API_KEY="your-api-key"

curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -H "X-API-Key: $API_KEY" \
  -d '{
    "messages": [{"role": "user", "content": "Hi"}],
    "model": "phi3:mini"
  }'
```

**V√°rhat√≥ v√°laszid≈ë: 2-5 m√°sodperc! ‚ö°**

## üöÄ Tov√°bbi optimaliz√°ci√≥

### 1. Kisebb modell haszn√°lata

**Gyorsabb modellek:**
- `phi3:mini` - 2-5 m√°sodperc ‚ö°
- `llama3.1:8b` - 5-10 m√°sodperc
- `mistral:7b` - 5-10 m√°sodperc

### 2. Stream haszn√°lata

**Val√≥s idej≈± v√°lasz (nem p√∂rgeti a CPU-t):**
```bash
curl -X POST http://localhost:8000/api/chat/stream \
  -H "Content-Type: application/json" \
  -H "X-API-Key: $API_KEY" \
  -d '{
    "messages": [{"role": "user", "content": "Hi"}],
    "model": "phi3:mini"
  }'
```

### 3. K√∂rnyezeti v√°ltoz√≥k

**Optimaliz√°lt `.env` f√°jl:**
```env
# CPU optimaliz√°ci√≥
OLLAMA_NUM_THREADS=8

# Gyorsabb v√°laszhoz
DEFAULT_MODEL=phi3:mini
```

## üìä V√°laszid≈ë v√°rakoz√°s

**Most m√°r gyorsabban:**
- `phi3:mini`: **2-5 m√°sodperc** (el≈ëtte: 30-180+ sec)
- `llama3.1:8b`: **5-10 m√°sodperc** (el≈ëtte: 60-180+ sec)
- `mistral:7b`: **5-10 m√°sodperc** (el≈ëtte: 60-180+ sec)

## ‚úÖ √ñsszegz√©s

1. ‚úÖ **Thread sz√°m**: 64 ‚Üí 8 (gyorsabb)
2. ‚úÖ **Context m√©ret**: 2048 ‚Üí 512 (gyorsabb)
3. ‚úÖ **Token limit**: 100 (gyorsabb v√°lasz)
4. ‚úÖ **mlock**: False (gyorsabb)

**Most m√°r gyorsabban m≈±k√∂dik, mint az `ollama run`! ‚ö°**

---

**Most m√°r optimaliz√°lt √©s gyors! üöÄ**

