# üêß Linux telep√≠t√©si √∫tmutat√≥ - L√©p√©sr≈ël l√©p√©sre

## üìã El≈ëfelt√©telek ellen≈ërz√©se

### ‚ö†Ô∏è FONTOS: Docker k√∂rnyezet

Ha **Docker m√°r telep√≠tve van** a g√©peden √©s m√°s programok is haszn√°lj√°k:
- ‚úÖ **NEM** telep√≠tj√ºk √∫jra a Docker-t
- ‚úÖ **NEM** √°ll√≠tjuk le a megl√©v≈ë kont√©nereket
- ‚úÖ Haszn√°lhatod a **Docker telep√≠t√©si opci√≥t** (al√°bb)
- ‚úÖ Vagy folytathatod a **hagyom√°nyos telep√≠t√©ssel** (Python virtu√°lis k√∂rnyezet)

### 1. l√©p√©s: Rendszerfriss√≠t√©s

```bash
sudo apt update
sudo apt upgrade -y
```

### 2. l√©p√©s: Telep√≠t√©si m√≥dszer kiv√°laszt√°sa

**V√°lassz egyet a kett≈ë k√∂z√ºl:**

#### Opci√≥ A: Docker telep√≠t√©s (aj√°nlott, ha Docker m√°r van)
- ‚úÖ Izol√°lt k√∂rnyezet
- ‚úÖ K√∂nny≈± karbantart√°s
- ‚úÖ Nem √©rinti a megl√©v≈ë rendszert
- üëâ [Ugr√°s a Docker telep√≠t√©shez](#-docker-telep√≠t√©s-opci√≥-a)

#### Opci√≥ B: Hagyom√°nyos telep√≠t√©s (Python virtu√°lis k√∂rnyezet)
- ‚úÖ Megl√©v≈ë `ai_venv` virtu√°lis k√∂rnyezet haszn√°lata
- ‚úÖ K√∂zvetlen hozz√°f√©r√©s
- ‚úÖ Egyszer≈±bb hibakeres√©s
- üëâ [Ugr√°s a hagyom√°nyos telep√≠t√©shez](#-hagyom√°nyos-telep√≠t√©s-opci√≥-b)

### 3. l√©p√©s: Git telep√≠t√©se/ellen≈ërz√©se

```bash
# Git ellen≈ërz√©se
git --version

# Ha nincs telep√≠tve:
sudo apt install git -y
```

---

## üê≥ Docker telep√≠t√©s (Opci√≥ A)

> **Megjegyz√©s:** Ez a m√≥dszer **nem telep√≠ti √∫jra a Docker-t** √©s **nem √°ll√≠tja le** a megl√©v≈ë kont√©nereket. 
> Csak √∫j kont√©nereket hoz l√©tre a ZedinArkManager sz√°m√°ra.

### 1. l√©p√©s: Docker ellen≈ërz√©se

```bash
# Docker ellen≈ërz√©se (ha m√°r telep√≠tve van)
docker --version

# Docker Compose ellen≈ërz√©se
docker-compose --version

# Megl√©v≈ë kont√©nerek list√°z√°sa (nem fogja ≈ëket megv√°ltoztatni)
docker ps
```

### 2. l√©p√©s: Repository kl√≥noz√°sa

```bash
git clone https://github.com/zedinke/ZedinArkManager.git
cd ZedinArkManager
```

### 3. l√©p√©s: Docker telep√≠t≈ë futtat√°sa

```bash
cd installers
chmod +x docker-install.sh
./docker-install.sh
cd ..
```

### 4. l√©p√©s: Docker Compose build √©s ind√≠t√°s

```bash
# Kont√©nerek build-el√©se √©s ind√≠t√°sa
cd installers
docker-compose up -d --build

# Logok k√∂vet√©se
docker-compose logs -f
```

### 5. l√©p√©s: Modell telep√≠t√©se

```bash
# Ollama kont√©nerbe bel√©p√©s √©s modell telep√≠t√©se
docker-compose exec ollama ollama pull llama3.1:8b

# Ez id≈ëbe telhet (~4-5GB let√∂lt√©s)
```

### 6. l√©p√©s: Ellen≈ërz√©s

```bash
# Health check
curl http://localhost:8000/health

# Kont√©nerek √°llapota
docker-compose ps
```

**Tov√°bbi inform√°ci√≥k:** L√°sd `docs/DOCKER_INSTALL.md`

---

## üöÄ Hagyom√°nyos telep√≠t√©s (Opci√≥ B)

### 1. l√©p√©s: Repository kl√≥noz√°sa

```bash
# Kl√≥nozd le a repository-t
git clone https://github.com/zedinke/ZedinArkManager.git

# L√©pj be a mapp√°ba
cd ZedinArkManager
```

### 2. l√©p√©s: Python virtu√°lis k√∂rnyezet aktiv√°l√°sa

**A megl√©v≈ë `ai_venv` virtu√°lis k√∂rnyezet haszn√°lata:**

```bash
# Aktiv√°ld a megl√©v≈ë virtu√°lis k√∂rnyezetet
source ai_venv/bin/activate

# Ellen≈ërz√©s - a prompt el√© ker√ºlj√∂n a (ai_venv)
# P√©lda: (ai_venv) user@server:~/ZedinArkManager$
```

**Ha nincs m√©g `ai_venv` virtu√°lis k√∂rnyezet:**

```bash
# Virtu√°lis k√∂rnyezet l√©trehoz√°sa (ha m√©g nincs)
python3 -m venv ai_venv

# Aktiv√°l√°s
source ai_venv/bin/activate
```

### 3. l√©p√©s: Automatikus telep√≠t≈ë futtat√°sa

```bash
# Telep√≠t≈ë script futtathat√≥v√° t√©tele
chmod +x installers/install.sh

# Telep√≠t≈ë futtat√°sa (virtu√°lis k√∂rnyezetben!)
./installers/install.sh
```

**Mit csin√°l a telep√≠t≈ë?**
- ‚úÖ L√©trehozza a sz√ºks√©ges mapp√°kat (`logs`, `data`, `projects`)
- ‚úÖ Telep√≠ti a Python f√ºgg≈ës√©geket a virtu√°lis k√∂rnyezetbe
- ‚úÖ Ellen≈ërzi az Ollama telep√≠t√©s√©t
- ‚úÖ L√©trehozza a `.env` konfigur√°ci√≥s f√°jlt

### 4. l√©p√©s: Ollama telep√≠t√©se (ha m√©g nincs)

**Ha m√©g nincs telep√≠tve az Ollama:**

```bash
# Ollama telep√≠t√©se
curl https://ollama.com/install.sh | sh
```

**Ellen≈ërz√©s:**

```bash
# Ollama verzi√≥ ellen≈ërz√©se
ollama --version
```

### 5. l√©p√©s: Ollama ind√≠t√°sa

**Opci√≥ 1: H√°t√©rben ind√≠t√°s (aj√°nlott)**

```bash
# Ollama ind√≠t√°sa h√°t√©rben
nohup ollama serve > logs/ollama.log 2>&1 &

# Ellen≈ërz√©s, hogy fut-e
curl http://localhost:11434/api/tags
```

**Opci√≥ 2: El≈ët√©rben ind√≠t√°s**

```bash
# Ollama ind√≠t√°sa el≈ët√©rben (Ctrl+C-vel le√°ll√≠that√≥)
ollama serve
```

### 6. l√©p√©s: Modell telep√≠t√©se

**Fontos:** A modell telep√≠t√©shez az Ollama-nak futnia kell!

```bash
# A magyarul j√≥l besz√©l≈ë modell let√∂lt√©se (~4-5GB, ez id≈ëbe telhet)
ollama pull llama3.1:8b

# Ellen≈ërz√©s
ollama list
```

**Alternat√≠v modellek (ha sz√ºks√©ges):**

```bash
ollama pull codellama       # K√≥d-gener√°l√°sra optimaliz√°lt
ollama pull mistral         # Kisebb, gyorsabb modell
ollama pull deepseek-coder  # Code generation modell
```

### 7. l√©p√©s: K√∂rnyezeti v√°ltoz√≥k be√°ll√≠t√°sa

**Szerkeszd a `.env` f√°jlt (ha sz√ºks√©ges):**

```bash
nano .env
```

**Alap√©rtelmezett √©rt√©kek (√°ltal√°ban ezek j√≥k):**

```env
# Ollama be√°ll√≠t√°sok
OLLAMA_URL=http://localhost:11434
DEFAULT_MODEL=llama3.1:8b

# Projekt be√°ll√≠t√°sok
PROJECT_BASE_PATH=.

# Optimaliz√°ci√≥ (opcion√°lis - a te szerveredhez):
OLLAMA_NUM_GPU_LAYERS=      # Ha van GPU, pl: 35
OLLAMA_NUM_THREADS=32       # 32 maghoz = 32 sz√°l
```

**Vagy k√∂rnyezeti v√°ltoz√≥k√©nt (ha nem haszn√°lod a .env f√°jlt):**

```bash
export OLLAMA_URL="http://localhost:11434"
export DEFAULT_MODEL="llama3.1:8b"
export PROJECT_BASE_PATH="."
export OLLAMA_NUM_THREADS="32"
```

### 8. l√©p√©s: Rendszer ind√≠t√°sa

**Fontos:** A virtu√°lis k√∂rnyezetnek aktiv√°lva kell lennie!

```bash
# Gy≈ëz≈ëdj meg r√≥la, hogy a virtu√°lis k√∂rnyezet akt√≠v
source ai_venv/bin/activate

# Ind√≠t√≥ script futtathat√≥v√° t√©tele
chmod +x start.sh

# Rendszer ind√≠t√°sa (virtu√°lis k√∂rnyezetben!)
./start.sh
```

**Mit csin√°l az ind√≠t√≥ script?**
- ‚úÖ Ellen≈ërzi, hogy az Ollama fut-e
- ‚úÖ Ellen≈ërzi a modellek telep√≠t√©s√©t (ha nincs, k√©rdezi, hogy telep√≠tse-e)
- ‚úÖ Ellen≈ërzi a Python f√ºgg≈ës√©geket
- ‚úÖ Bet√∂lti a k√∂rnyezeti v√°ltoz√≥kat (`.env` f√°jlb√≥l)
- ‚úÖ Ind√≠tja a FastAPI szervert

### 9. l√©p√©s: Ellen≈ërz√©s, hogy minden m≈±k√∂dik

**1. Health check (termin√°lb√≥l):**

```bash
curl http://localhost:8000/health
```

**V√°rhat√≥ v√°lasz:**
```json
{
  "status": "healthy",
  "ollama_connected": true,
  "base_path": ".",
  "default_model": "llama3.1:8b"
}
```

**2. API dokument√°ci√≥ megnyit√°sa (b√∂ng√©sz≈ëben):**

```
http://localhost:8000/docs
```

**3. Telep√≠tett modellek ellen≈ërz√©se:**

```bash
curl http://localhost:8000/api/models
```

---

## ‚úÖ Telep√≠t√©s k√©sz!

Ha minden l√©p√©s sikeres volt, a rendszer most fut √©s el√©rhet≈ë:

- **API**: http://localhost:8000
- **API Dokument√°ci√≥**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/health

---

## üîß K√©s≈ëbbi haszn√°lat (hagyom√°nyos telep√≠t√©s)

### Rendszer ind√≠t√°sa

**Fontos:** Mindig aktiv√°ld a virtu√°lis k√∂rnyezetet!

```bash
# Aktiv√°ld a virtu√°lis k√∂rnyezetet
source ai_venv/bin/activate

# Rendszer ind√≠t√°sa
./start.sh
```

### Rendszer le√°ll√≠t√°sa

```bash
# Ctrl+C a termin√°lban, ahol fut
# Vagy ha h√°t√©rben fut:
ps aux | grep "python.*main.py"
kill <PID>
```

### H√°t√©rben ind√≠t√°s

```bash
# Aktiv√°ld a virtu√°lis k√∂rnyezetet
source ai_venv/bin/activate

# Ollama h√°t√©rben (ha m√©g nem fut)
nohup ollama serve > logs/ollama.log 2>&1 &

# FastAPI h√°t√©rben
nohup python3 main.py > logs/app.log 2>&1 &

# PID ment√©se (k√©s≈ëbb le√°ll√≠t√°shoz)
echo $! > logs/api.pid
```

---

## üîç Hibaelh√°r√≠t√°s

### Virtu√°lis k√∂rnyezet probl√©m√°k

```bash
# Aktiv√°l√°s ellen≈ërz√©se
which python3
# V√°lasz: /path/to/ai_venv/bin/python3

# Ha nem j√≥, aktiv√°ld √∫jra
source ai_venv/bin/activate

# F√ºgg≈ës√©gek √∫jratelep√≠t√©se
pip3 install -r installers/requirements.txt
```

### Ollama nem fut

```bash
# Ellen≈ërz√©s
curl http://localhost:11434/api/tags

# Ha nem fut, ind√≠tsd el:
ollama serve

# Vagy h√°t√©rben:
nohup ollama serve > logs/ollama.log 2>&1 &
```

### Ollama kapcsolati hiba

```bash
# Ellen≈ërizd, hogy az Ollama fut-e
ps aux | grep ollama

# Ha nem fut, ind√≠tsd el
ollama serve &

# V√°rj 2-3 m√°sodpercet, majd pr√≥b√°ld √∫jra
sleep 3
curl http://localhost:11434/api/tags
```

### Modell nincs telep√≠tve

```bash
# Modell telep√≠t√©se (Ollama-nak futnia kell!)
ollama pull llama3.1:8b

# Ellen≈ërz√©s
ollama list
```

### Python f√ºgg≈ës√©gek hi√°nyoznak

```bash
# Aktiv√°ld a virtu√°lis k√∂rnyezetet
source ai_venv/bin/activate

# F√ºgg≈ës√©gek telep√≠t√©se
pip3 install -r installers/requirements.txt

# Ha hiba van, pr√≥b√°ld:
pip3 install --upgrade pip
pip3 install -r installers/requirements.txt --force-reinstall
```

### Port m√°r haszn√°latban

```bash
# Melyik process haszn√°lja a 8000-es portot?
sudo lsof -i :8000

# Vagy
sudo netstat -tulpn | grep :8000

# Process le√°ll√≠t√°sa (ha sz√ºks√©ges)
sudo kill -9 <PID>
```

### Jogosults√°g hib√°k

```bash
# Scriptek futtathat√≥v√° t√©tele
chmod +x start.sh
chmod +x installers/install.sh

# Mapp√°k jogosults√°gai
chmod -R 755 logs data projects
```

### Log f√°jlok ellen≈ërz√©se

```bash
# Alkalmaz√°s logok
tail -f logs/app.log

# Ollama logok
tail -f logs/ollama.log

# Hib√°k keres√©se
grep -i error logs/app.log
```

---

## üìù Tov√°bbi inform√°ci√≥k

- **API dokument√°ci√≥**: http://localhost:8000/docs
- **Projekt strukt√∫ra**: `docs/PROJECT_STRUCTURE.md`
- **Haszn√°lati √∫tmutat√≥**: `docs/USAGE_GUIDE.md`
- **Docker telep√≠t√©s**: `docs/DOCKER_INSTALL.md`
- **GitHub repository**: https://github.com/zedinke/ZedinArkManager

---

## üéâ Sikeres telep√≠t√©s!

Ha minden l√©p√©s sikeres volt, a rendszer most fut √©s haszn√°latra k√©sz!

**Els≈ë l√©p√©sek:**
1. Aktiv√°ld a virtu√°lis k√∂rnyezetet: `source ai_venv/bin/activate`
2. Nyisd meg: http://localhost:8000/docs
3. Pr√≥b√°ld ki a `/api/chat` endpoint-ot
4. Gener√°lj k√≥dot a `/api/generate` endpoint-tal

**J√≥ munk√°t! üöÄ**
