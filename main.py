"""
AI Coding Assistant - FastAPI Backend
Helyi LLM modellekkel m≈±k√∂d≈ë k√≥dol√°si asszisztens
"""
from fastapi import FastAPI, HTTPException, Security
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import uvicorn
import os
import logging
from pathlib import Path

from core.llm_service import LLMService
from core.file_manager import FileManager
from core.response_cache import ResponseCache
from core.project_manager import ProjectManager
from core.conversation_memory import ConversationMemory
from core.auth import api_key_manager, verify_api_key
from core.gpu_manager import gpu_manager
from core.distributed_computing import distributed_network, ComputeNode, NodeStatus
from modules.code_generator import CodeGenerator
from modules.project_context import ProjectContext

# Logging be√°ll√≠t√°s
log_dir = Path("logs")
log_dir.mkdir(exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/app.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Inicializ√°l√°s
BASE_PATH = os.getenv("PROJECT_BASE_PATH", ".")
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434")
DEFAULT_MODEL = os.getenv("DEFAULT_MODEL", "llama3.1:8b")

NUM_GPU_LAYERS = os.getenv("OLLAMA_NUM_GPU_LAYERS")
if NUM_GPU_LAYERS:
    NUM_GPU_LAYERS = int(NUM_GPU_LAYERS)
else:
    # GPU manager automatikus detekt√°l√°s
    NUM_GPU_LAYERS = gpu_manager.get_ollama_gpu_layers()

NUM_THREADS = os.getenv("OLLAMA_NUM_THREADS")
if NUM_THREADS:
    NUM_THREADS = int(NUM_THREADS)
else:
    NUM_THREADS = None

# Szolg√°ltat√°sok inicializ√°l√°sa
llm_service = LLMService(
    base_url=OLLAMA_URL, 
    default_model=DEFAULT_MODEL,
    num_gpu_layers=NUM_GPU_LAYERS,
    num_threads=NUM_THREADS
)
file_manager = FileManager(base_path=BASE_PATH)
project_manager = ProjectManager(base_path="projects")
code_generator = CodeGenerator(llm_service, file_manager, project_manager)
project_context = ProjectContext(file_manager, base_path=BASE_PATH)
conversation_memory = ConversationMemory(project_name="global", storage_dir="./data/memory")
response_cache = ResponseCache(cache_dir="./data/cache", ttl=1800)

# Szerver automatikus regisztr√°l√°sa a distributed h√°l√≥zatba
def register_server_as_node():
    """Szerver automatikus regisztr√°l√°sa compute node-k√©nt"""
    try:
        import socket
        import multiprocessing
        
        # Szerver inform√°ci√≥k gy≈±jt√©se
        gpu_count = gpu_manager.get_gpu_count()
        gpu_info = gpu_manager.get_all_gpus_status() if gpu_count > 0 else []
        total_gpu_memory = sum(gpu.get("memory_total", 0) for gpu in gpu_info) if gpu_info else 0
        cpu_cores = multiprocessing.cpu_count()
        
        # Szerver modellek lek√©r√©se
        try:
            server_models = llm_service.list_models()
        except:
            server_models = []
        
        # Hostname vagy IP
        hostname = socket.gethostname()
        try:
            host_ip = socket.gethostbyname(hostname)
        except:
            host_ip = "localhost"
        
        # Node regisztr√°l√°sa
        server_node = distributed_network.register_node(
            node_id=f"server-{hostname}",
            user_id="server",
            name=f"Server - {hostname} ({host_ip})",
            ollama_url=OLLAMA_URL,
            api_key=None,
            gpu_count=gpu_count,
            gpu_memory=total_gpu_memory,
            cpu_cores=cpu_cores
        )
        
        # Modellek friss√≠t√©se
        if server_models:
            distributed_network.update_node_status(
                node_id=server_node.node_id,
                status=NodeStatus.ONLINE,
                available_models=server_models
            )
        
        logger.info(f"Server registered as compute node: {server_node.node_id} "
                   f"(GPU: {gpu_count}, CPU: {cpu_cores}, Models: {len(server_models)})")
        
        return server_node
    except Exception as e:
        logger.warning(f"Failed to register server as node: {e}")
        return None

# Szerver regisztr√°l√°sa ind√≠t√°skor
server_node = register_server_as_node()

# Distributed network statisztik√°k ki√≠r√°sa ind√≠t√°skor
def print_distributed_network_info():
    """Ki√≠rja a distributed network inform√°ci√≥kat ind√≠t√°skor"""
    try:
        stats = distributed_network.get_network_stats()
        available_nodes = distributed_network.get_available_nodes(ignore_model_filter=True)
        
        print("\n" + "="*60)
        print("üåê DISTRIBUTED COMPUTING NETWORK")
        print("="*60)
        print(f"üìä √ñsszes regisztr√°lt csom√≥pont: {stats['total_nodes']}")
        print(f"‚úÖ Online csom√≥pontok: {stats['online_nodes']}")
        print(f"üíª √ñsszes GPU: {stats['total_gpu']}")
        print(f"üß† √ñsszes GPU mem√≥ria: {stats['total_gpu_memory_gb']:.2f} GB")
        print(f"‚öôÔ∏è  √ñsszes CPU mag: {stats['total_cpu_cores']}")
        print(f"üîÑ Akt√≠v feladatok: {stats['active_tasks']}")
        print(f"‚úÖ Befejezett feladatok: {stats['completed_tasks']}")
        print("-"*60)
        
        if available_nodes:
            print(f"üöÄ Egy k√©r√©s {len(available_nodes)} csom√≥ponton lesz futtatva:")
            for i, node in enumerate(available_nodes, 1):
                models_info = f"{len(node.available_models)} modell" if node.available_models else "modell info n√©lk√ºl"
                print(f"   {i}. {node.name}")
                print(f"      - ID: {node.node_id}")
                print(f"      - GPU: {node.gpu_count}, CPU: {node.cpu_cores}, {models_info}")
                print(f"      - Terhel√©s: {node.current_load*100:.1f}%, V√°laszid≈ë: {node.response_time:.0f}ms")
        else:
            print("‚ö†Ô∏è  Nincs el√©rhet≈ë csom√≥pont a distributed computing-hez")
            print("   (A k√©r√©sek lok√°lisan lesznek feldolgozva)")
        
        print("="*60 + "\n")
    except Exception as e:
        logger.warning(f"Failed to print distributed network info: {e}")

# Network info ki√≠r√°sa ind√≠t√°skor
print_distributed_network_info()

# FastAPI app
app = FastAPI(
    title="AI Coding Assistant",
    description="Helyi LLM modellekkel m≈±k√∂d≈ë k√≥dol√°si asszisztens",
    version="1.0.0"
)

# CORS be√°ll√≠t√°s
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # √âles k√∂rnyezetben korl√°tozd!
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pydantic modellek
class ChatMessage(BaseModel):
    role: str = Field(..., description="√úzenet szerepe: user, assistant, system")
    content: str = Field(..., description="√úzenet tartalma")


class ChatRequest(BaseModel):
    messages: List[ChatMessage] = Field(..., description="Chat √ºzenetek")
    model: Optional[str] = Field(None, description="LLM modell neve")
    temperature: float = Field(0.5, ge=0.0, le=2.0, description="Kreativit√°s")
    auto_save_code: bool = Field(True, description="Automatikus k√≥d ment√©s")
    use_cache: bool = Field(True, description="Cache haszn√°lata")


class GenerateCodeRequest(BaseModel):
    prompt: str = Field(..., description="K√≥d gener√°l√°si prompt")
    language: str = Field("python", description="Programoz√°si nyelv")
    context_files: Optional[List[str]] = Field(None, description="Kontextus f√°jlok")
    model: Optional[str] = Field(None, description="LLM modell neve")
    auto_save: bool = Field(True, description="Automatikus ment√©s f√°jlba")
    file_path: Optional[str] = Field(None, description="F√°jl √∫tvonal")
    use_cache: bool = Field(True, description="Cache haszn√°lata")


class EditCodeRequest(BaseModel):
    file_path: str = Field(..., description="Szerkesztend≈ë f√°jl")
    instruction: str = Field(..., description="Szerkeszt√©si utas√≠t√°s")
    model: Optional[str] = Field(None, description="LLM modell neve")


class RefactorRequest(BaseModel):
    file_path: str = Field(..., description="Refaktor√°land√≥ f√°jl")
    refactor_type: str = Field("clean", description="Refaktor√°l√°s t√≠pusa")
    model: Optional[str] = Field(None, description="LLM modell neve")


class WriteFileRequest(BaseModel):
    file_path: str = Field(..., description="F√°jl √∫tvonal")
    content: str = Field(..., description="F√°jl tartalma")
    create_dirs: bool = Field(True, description="K√∂nyvt√°rak l√©trehoz√°sa")


class CreateProjectRequest(BaseModel):
    name: str = Field(..., description="Projekt neve")
    type: str = Field("general", description="Projekt t√≠pus")
    description: str = Field("", description="Projekt le√≠r√°s")


class SelectProjectRequest(BaseModel):
    name: str = Field(..., description="Projekt neve")


class VisionRequest(BaseModel):
    image: str = Field(..., description="Base64 encoded image")
    prompt: str = Field(..., description="Prompt for image analysis")
    model: Optional[str] = Field(None, description="Vision model name (e.g., llava)")


# API Endpoints
@app.get("/")
async def root():
    """Root endpoint (autentik√°ci√≥ n√©lk√ºl)"""
    return {
        "message": "AI Coding Assistant API",
        "version": "1.0.0",
        "endpoints": {
            "health": "/health",
            "docs": "/docs",
            "models": "/api/models",
            "chat": "/api/chat (POST only)",
            "generate": "/api/generate",
            "edit": "/api/edit",
            "explain": "/api/explain",
            "refactor": "/api/refactor",
            "files": "/api/files",
            "projects": "/api/projects",
            "auth": "/api/auth"
        },
        "auth_enabled": os.getenv("ENABLE_AUTH", "false").lower() == "true",
        "note": "A b√∂ng√©sz≈ëben haszn√°ld a /docs oldalt az API tesztel√©s√©hez! Az /api/chat csak POST k√©r√©st fogad el."
    }


@app.get("/health")
async def health_check():
    """Health check endpoint (autentik√°ci√≥ n√©lk√ºl)"""
    ollama_connected = llm_service.check_connection()
    gpu_count = gpu_manager.get_gpu_count()
    
    # Szerver node √°llapot friss√≠t√©se
    if server_node:
        try:
            server_models = llm_service.list_models()
            distributed_network.update_node_status(
                node_id=server_node.node_id,
                status=NodeStatus.ONLINE if ollama_connected else NodeStatus.OFFLINE,
                available_models=server_models,
                current_load=0.0  # TODO: val√≥s terhel√©s m√©r√©se
            )
        except:
            pass
    
    return {
        "status": "healthy" if ollama_connected else "degraded",
        "ollama_connected": ollama_connected,
        "base_path": BASE_PATH,
        "default_model": DEFAULT_MODEL,
        "auth_enabled": os.getenv("ENABLE_AUTH", "false").lower() == "true",
        "gpu_count": gpu_count,
        "gpu_layers": NUM_GPU_LAYERS,
        "distributed_network": {
            "server_registered": server_node is not None,
            "total_nodes": len(distributed_network.nodes),
            "online_nodes": len([n for n in distributed_network.nodes.values() if n.status == NodeStatus.ONLINE])
        }
    }


@app.get("/api/models")
async def list_models(api_key: Optional[str] = Security(verify_api_key)):
    """Telep√≠tett modellek list√°z√°sa"""
    try:
        models = llm_service.list_models()
        return {
            "models": models,
            "default": DEFAULT_MODEL,
            "available": len(models) > 0
        }
    except Exception as e:
        logger.error(f"Error listing models: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/chat")
async def chat(request: ChatRequest, api_key: Optional[str] = Security(verify_api_key)):
    """Chat endpoint"""
    try:
        messages = [
            {"role": msg.role, "content": msg.content}
            for msg in request.messages
        ]
        
        has_system = any(msg.get("role") == "system" for msg in messages)
        if not has_system:
            last_user_msg = messages[-1]["content"].lower() if messages else ""
            code_keywords = ["hoz", "k√©sz√≠ts", "create", "generate", "make", "√≠rd", "write"]
            
            if any(keyword in last_user_msg for keyword in code_keywords):
                system_prompt = """Te egy professzion√°lis programoz√≥ vagy. 
Amikor k√≥dot vagy f√°jlt k√©rnek t≈ëled, MINDIG gener√°ld a teljes, m≈±k√∂d≈ë k√≥dot k√≥d blokkban (```), ne csak √≠rd le hogyan kell.
A k√≥dot mindig ``` nyelv form√°tumban add vissza."""
                messages.insert(0, {"role": "system", "content": system_prompt})
        
        # Distributed computing haszn√°lata, ha be van kapcsolva √©s van el√©rhet≈ë csom√≥pont
        # Alap√©rtelmezetten be van kapcsolva, ha van el√©rhet≈ë csom√≥pont
        # ignore_model_filter=True: minden modell haszn√°lja az √∂sszes beregisztr√°lt er≈ëforr√°st
        available_nodes = distributed_network.get_available_nodes(
            model=request.model,
            ignore_model_filter=True  # Minden csom√≥pontot haszn√°l, f√ºggetlen√ºl a modellt≈ël
        )
        use_distributed_computing = len(available_nodes) > 0
        
        # Logol√°s: mely csom√≥pontok lesznek haszn√°lva
        if use_distributed_computing:
            node_info = [f"{n.node_id} ({n.name})" for n in available_nodes]
            logger.info(f"Distributed computing enabled: {len(available_nodes)} nodes available: {node_info}")
        
        if use_distributed_computing:
            try:
                # Distributed h√°l√≥zat haszn√°lata - minden el√©rhet≈ë csom√≥ponton p√°rhuzamosan
                # Ez BELE√âRTI a szerver node-ot is, √≠gy a szerver er≈ëforr√°sai is haszn√°l√≥dnak
                user_id = api_key or "anonymous"
                logger.info(f"üöÄ Using distributed computing with {len(available_nodes)} nodes: {[n.node_id for n in available_nodes]}")
                response = await distributed_network.distribute_task(
                    user_id=user_id,
                    model=request.model or DEFAULT_MODEL,
                    messages=messages,
                    use_all_nodes=True  # Minden el√©rhet≈ë csom√≥pontot haszn√°l (szerver + felhaszn√°l√≥i g√©pek)
                )
                logger.info(f"‚úÖ Distributed computing completed: {len(available_nodes)} nodes used")
            except Exception as e:
                logger.warning(f"Distributed computing failed, falling back to local: {e}")
                # Fallback lok√°lis LLM service-re
                response = llm_service.chat(
                    messages=messages,
                    model=request.model,
                    temperature=request.temperature
                )
        else:
            # Hagyom√°nyos lok√°lis feldolgoz√°s
            cache_key = None
            if request.use_cache and not has_system and len(messages) == 1:
                last_msg = messages[-1]["content"] if messages else ""
                cached_response = response_cache.get(last_msg, request.model, request.temperature)
                if cached_response:
                    response = cached_response
                else:
                    response = llm_service.chat(
                        messages=messages,
                        model=request.model,
                        temperature=request.temperature
                    )
                    response_cache.set(last_msg, response, request.model, request.temperature)
            else:
                response = llm_service.chat(
                    messages=messages,
                    model=request.model,
                    temperature=request.temperature
                )
        
        last_user_message = messages[-1]["content"] if messages and messages[-1].get("role") == "user" else ""
        
        save_result = code_generator.extract_and_save_code_from_chat(
            chat_response=response,
            auto_save=request.auto_save_code,
            user_message=last_user_message
        )
        
        result = {
            "response": response,
            "model": request.model or DEFAULT_MODEL
        }
        
        if save_result.get("files_saved"):
            result["files_saved"] = save_result["files_saved"]
            result["code_detected"] = True
        else:
            result["code_detected"] = save_result.get("code_found", False)
        
        return result
    except Exception as e:
        logger.error(f"Chat error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/chat/stream")
async def chat_stream(request: ChatRequest, api_key: Optional[str] = Security(verify_api_key)):
    """Stream chat endpoint"""
    try:
        messages = [
            {"role": msg.role, "content": msg.content}
            for msg in request.messages
        ]
        
        async def generate():
            async for chunk in llm_service.chat_stream(
                messages=messages,
                model=request.model,
                temperature=request.temperature
            ):
                yield f"data: {chunk}\n\n"
        
        return StreamingResponse(
            generate(),
            media_type="text/event-stream"
        )
    except Exception as e:
        logger.error(f"Chat stream error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/generate")
async def generate_code(request: GenerateCodeRequest, api_key: Optional[str] = Security(verify_api_key)):
    """K√≥d gener√°l√°s"""
    try:
        cached_code = None
        if request.use_cache and not request.context_files:
            cached_code = response_cache.get(request.prompt, request.model, 0.2)
        
        if cached_code:
            result = {
                "code": cached_code,
                "explanation": "K√≥d cache-b≈ël",
                "file_path": None,
                "error": None,
                "cached": True
            }
        else:
            result = code_generator.generate_code(
                prompt=request.prompt,
                language=request.language,
                context_files=request.context_files,
                model=request.model,
                auto_save=request.auto_save,
                file_path=request.file_path
            )
            if result.get("code") and request.use_cache and not request.context_files:
                response_cache.set(request.prompt, result["code"], request.model, 0.2)
        
        if result.get("error"):
            raise HTTPException(status_code=400, detail=result["error"])
        
        return {
            "code": result["code"],
            "explanation": result.get("explanation"),
            "file_path": result.get("file_path"),
            "language": request.language,
            "saved": result.get("file_path") is not None
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Generate code error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/edit")
async def edit_code(request: EditCodeRequest, api_key: Optional[str] = Security(verify_api_key)):
    """K√≥d szerkeszt√©s"""
    try:
        result = code_generator.edit_code(
            file_path=request.file_path,
            instruction=request.instruction,
            model=request.model
        )
        
        if result.get("error"):
            raise HTTPException(status_code=400, detail=result["error"])
        
        return {
            "code": result["code"],
            "file_path": request.file_path
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Edit code error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/explain/{file_path:path}")
async def explain_code(file_path: str, model: Optional[str] = None, api_key: Optional[str] = Security(verify_api_key)):
    """K√≥d magyar√°zata"""
    try:
        result = code_generator.explain_code(
            file_path=file_path,
            model=model
        )
        
        if result.get("error"):
            raise HTTPException(status_code=400, detail=result["error"])
        
        return {
            "explanation": result["explanation"],
            "file_path": file_path
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Explain code error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/vision")
async def analyze_image(request: VisionRequest, api_key: Optional[str] = Security(verify_api_key)):
    """K√©p √©rtelmez√©se vision modellel (Ollama llava)"""
    try:
        import base64
        from io import BytesIO
        from PIL import Image
        import requests
        
        # Base64 k√©p dek√≥dol√°sa √©s valid√°l√°s
        try:
            # Tiszt√≠tsuk a base64 stringet (elt√°vol√≠tjuk a data:image prefix-et ha van)
            image_base64 = request.image
            if ',' in image_base64:
                image_base64 = image_base64.split(',', 1)[1]
            
            image_data = base64.b64decode(image_base64)
            image = Image.open(BytesIO(image_data))
            
            # K√©p inform√°ci√≥
            image_info = {
                "format": image.format,
                "size": image.size,
                "mode": image.mode
            }
            
            # Vision model (llava vagy m√°s vision model)
            model = request.model or "llava"
            
            # Ollama vision API h√≠v√°s
            # Az Ollama chat API t√°mogatja a k√©peket images array-ben
            ollama_url = f"{OLLAMA_URL}/api/chat"
            
            # Prompt el≈ëk√©sz√≠t√©se
            prompt = request.prompt or "Elemezd ezt a k√©pet r√©szletesen. √çrd le, mit l√°tsz, milyen objektumok, sz√≠nek, sz√∂vegek vannak rajta, √©s adj relev√°ns inform√°ci√≥kat."
            
            # Ollama chat API form√°tum k√©pekkel
            # Az Ollama API-ban az images array a message-en k√≠v√ºl van, vagy a content-ben lehet
            # Pr√≥b√°ljuk meg mindk√©t form√°tumot t√°mogatni
            payload = {
                "model": model,
                "messages": [
                    {
                        "role": "user",
                        "content": prompt,
                        "images": [image_base64]  # Base64 string array - ez az Ollama form√°tum
                    }
                ],
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "num_predict": 500,
                    "num_ctx": 4096  # Nagyobb context window a vision modellekre
                }
            }
            
            try:
                # Ollama API h√≠v√°s
                response = requests.post(
                    ollama_url,
                    json=payload,
                    timeout=120  # Vision modellekre hosszabb timeout
                )
                
                if response.status_code == 200:
                    data = response.json()
                    vision_response = data.get("message", {}).get("content", "")
                    
                    if vision_response:
                        return {
                            "response": vision_response,
                            "image_info": image_info,
                            "model": model,
                            "success": True
                        }
                    else:
                        raise Exception("Ollama v√°lasz √ºres")
                else:
                    # Ha a modell nincs telep√≠tve, pr√≥b√°ljuk meg ellen≈ërizni
                    if response.status_code == 404:
                        error_msg = f"Vision model ({model}) nincs telep√≠tve. Telep√≠tsd: ollama pull {model}"
                        logger.warning(error_msg)
                        return {
                            "response": f"{error_msg}\n\nK√©p inform√°ci√≥: {image_info['format']}, {image_info['size'][0]}x{image_info['size'][1]} pixel",
                            "image_info": image_info,
                            "model": model,
                            "success": False,
                            "error": "model_not_found"
                        }
                    else:
                        raise Exception(f"Ollama API error: {response.status_code} - {response.text}")
                        
            except requests.exceptions.RequestException as ollama_error:
                logger.error(f"Ollama vision API error: {ollama_error}")
                # Ha Ollama nem el√©rhet≈ë vagy a modell nincs, alapvet≈ë inform√°ci√≥t adunk vissza
                return {
                    "response": f"Ollama vision API nem el√©rhet≈ë vagy a {model} modell nincs telep√≠tve.\n\nK√©p inform√°ci√≥:\n- Form√°tum: {image_info['format']}\n- M√©ret: {image_info['size'][0]}x{image_info['size'][1]} pixel\n- M√≥d: {image_info['mode']}\n\nTelep√≠tsd a vision modelt: ollama pull {model}",
                    "image_info": image_info,
                    "model": model,
                    "success": False,
                    "error": str(ollama_error)
                }
            
        except Exception as img_error:
            logger.error(f"Image processing error: {img_error}")
            return {
                "response": f"K√©p feldolgoz√°si hiba: {str(img_error)}",
                "error": str(img_error),
                "success": False
            }
    except Exception as e:
        logger.error(f"Vision analysis error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/refactor")
async def refactor_code(request: RefactorRequest, api_key: Optional[str] = Security(verify_api_key)):
    """K√≥d refaktor√°l√°s"""
    try:
        result = code_generator.refactor_code(
            file_path=request.file_path,
            refactor_type=request.refactor_type,
            model=request.model
        )
        
        if result.get("error"):
            raise HTTPException(status_code=400, detail=result["error"])
        
        return {
            "code": result["code"],
            "changes": result.get("changes"),
            "refactor_type": request.refactor_type
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Refactor code error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# F√°jl m≈±veletek
@app.get("/api/files/{file_path:path}")
async def read_file_endpoint(file_path: str, api_key: Optional[str] = Security(verify_api_key)):
    """F√°jl olvas√°sa"""
    try:
        result = file_manager.read_file(file_path)
        
        if not result.get("exists"):
            raise HTTPException(status_code=404, detail=result.get("error", "File not found"))
        
        if result.get("error"):
            raise HTTPException(status_code=400, detail=result["error"])
        
        return {
            "content": result["content"],
            "file_path": file_path,
            "size": result.get("size", 0),
            "lines": result.get("lines", 0)
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Read file error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/files")
async def write_file_endpoint(request: WriteFileRequest, api_key: Optional[str] = Security(verify_api_key)):
    """F√°jl √≠r√°sa"""
    try:
        result = file_manager.write_file(
            file_path=request.file_path,
            content=request.content,
            create_dirs=request.create_dirs
        )
        
        if not result.get("success"):
            raise HTTPException(status_code=400, detail=result.get("error", "Write failed"))
        
        return {
            "success": True,
            "file_path": result.get("path", request.file_path)
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Write file error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/api/files/{file_path:path}")
async def delete_file_endpoint(file_path: str, api_key: Optional[str] = Security(verify_api_key)):
    """F√°jl t√∂rl√©se"""
    try:
        result = file_manager.delete_file(file_path)
        
        if not result.get("success"):
            raise HTTPException(status_code=400, detail=result.get("error", "Delete failed"))
        
        return {"success": True, "file_path": file_path}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Delete file error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/files")
async def list_files_endpoint(dir_path: str = ".", recursive: bool = False, api_key: Optional[str] = Security(verify_api_key)):
    """K√∂nyvt√°r tartalm√°nak list√°z√°sa"""
    try:
        result = file_manager.list_directory(dir_path, recursive=recursive)
        
        if result.get("error"):
            raise HTTPException(status_code=400, detail=result["error"])
        
        return {
            "files": result["files"],
            "directories": result["directories"],
            "path": dir_path
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"List files error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# Projekt m≈±veletek
@app.get("/api/projects")
async def list_projects(api_key: Optional[str] = Security(verify_api_key)):
    """Projektek list√°z√°sa"""
    try:
        projects = project_manager.list_projects()
        return {
            "projects": projects,
            "count": len(projects),
            "current": project_manager.current_project
        }
    except Exception as e:
        logger.error(f"List projects error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/projects/create")
async def create_project(request: CreateProjectRequest, api_key: Optional[str] = Security(verify_api_key)):
    """√öj projekt l√©trehoz√°sa"""
    try:
        result = project_manager.create_project(
            name=request.name,
            project_type=request.type,
            description=request.description
        )
        if not result.get("success"):
            raise HTTPException(status_code=400, detail=result.get("error"))
        return result
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Create project error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/projects/select")
async def select_project(request: SelectProjectRequest, api_key: Optional[str] = Security(verify_api_key)):
    """Projekt kiv√°laszt√°sa"""
    try:
        success = project_manager.set_current_project(request.name)
        if not success:
            raise HTTPException(status_code=404, detail=f"Projekt '{request.name}' nem tal√°lhat√≥")
        return {
            "status": "selected",
            "project": request.name,
            "project_info": project_manager.get_current_project()
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Select project error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/projects/current")
async def get_current_project(api_key: Optional[str] = Security(verify_api_key)):
    """Aktu√°lis projekt lek√©r√©se"""
    try:
        current = project_manager.get_current_project()
        if not current:
            return {"current": None, "message": "Nincs kiv√°lasztott projekt"}
        return {"current": current}
    except Exception as e:
        logger.error(f"Get current project error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/distributed/register")
async def register_compute_node(
    request: Dict[str, Any],
    api_auth: Optional[str] = Security(verify_api_key)
):
    """Compute node regisztr√°l√°sa a distributed network-be"""
    try:
        node = distributed_network.register_node(
            node_id=request.get("node_id"),
            user_id=request.get("user_id", "user"),
            name=request.get("name"),
            ollama_url=request.get("ollama_url"),
            api_key=request.get("api_key"),
            gpu_count=request.get("gpu_count", 0),
            gpu_memory=request.get("gpu_memory", 0),
            cpu_cores=request.get("cpu_cores", 0)
        )
        
        # Modellek lek√©r√©se (ha el√©rhet≈ë)
        try:
            import requests
            ollama_url = request.get("ollama_url")
            response = requests.get(f"{ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                data = response.json()
                models = [m["name"] for m in data.get("models", [])]
                distributed_network.update_node_status(
                    node_id=node.node_id,
                    status=NodeStatus.ONLINE,
                    available_models=models
                )
        except:
            pass
        
        logger.info(f"Compute node registered: {node.node_id} ({request.get('name')}) from {request.get('ollama_url')}")
        return {
            "success": True,
            "node_id": node.node_id,
            "message": f"Node {node.node_id} successfully registered"
        }
    except Exception as e:
        logger.error(f"Failed to register compute node: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/distributed/stats")
async def get_distributed_stats(api_key: Optional[str] = Security(verify_api_key)):
    """Distributed network statisztik√°k"""
    try:
        stats = distributed_network.get_network_stats()
        nodes = []
        for node in distributed_network.nodes.values():
            nodes.append({
                "node_id": node.node_id,
                "name": node.name,
                "status": node.status.value,
                "gpu_count": node.gpu_count,
                "cpu_cores": node.cpu_cores,
                "available_models": node.available_models,
                "current_load": node.current_load,
                "response_time": node.response_time
            })
        return {
            **stats,
            "nodes": nodes
        }
    except Exception as e:
        logger.error(f"Failed to get distributed stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/project/structure")
async def get_project_structure(max_depth: int = 3, api_key: Optional[str] = Security(verify_api_key)):
    """Projekt strukt√∫ra lek√©r√©se"""
    try:
        structure = project_context.get_project_structure(max_depth=max_depth)
        return structure
    except Exception as e:
        logger.error(f"Get project structure error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/project/context")
async def get_project_context_endpoint(file_path: Optional[str] = None, 
                              include_related: bool = True,
                              api_key: Optional[str] = Security(verify_api_key)):
    """Projekt kontextus lek√©r√©se"""
    try:
        if file_path:
            context = project_context.get_file_context(file_path, include_related)
        else:
            context = {
                "codebase": project_context.build_codebase_context(),
                "structure": project_context.get_project_structure()
            }
        return context
    except Exception as e:
        logger.error(f"Get project context error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/project/search")
async def search_files(query: str, limit: int = 10, api_key: Optional[str] = Security(verify_api_key)):
    """F√°jlok keres√©se"""
    try:
        files = project_context.get_relevant_files(query, limit=limit)
        return {
            "query": query,
            "files": files,
            "count": len(files)
        }
    except Exception as e:
        logger.error(f"Search files error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# Mem√≥ria √©s cache endpointok
@app.get("/api/memory/summary")
async def get_memory_summary(api_key: Optional[str] = Security(verify_api_key)):
    """Besz√©lget√©si mem√≥ria √∂sszefoglal√≥"""
    try:
        summary = conversation_memory.get_summary()
        return {
            "summary": summary,
            "message_count": len(conversation_memory.messages)
        }
    except Exception as e:
        logger.error(f"Get memory summary error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/memory/clear")
async def clear_memory(api_key: Optional[str] = Security(verify_api_key)):
    """Besz√©lget√©si mem√≥ria t√∂rl√©se"""
    try:
        conversation_memory.clear()
        return {"status": "cleared"}
    except Exception as e:
        logger.error(f"Clear memory error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/cache/clear")
async def clear_cache(api_key: Optional[str] = Security(verify_api_key)):
    """Response cache t√∂rl√©se"""
    try:
        response_cache.clear()
        return {"status": "cleared", "message": "Cache t√∂r√∂lve"}
    except Exception as e:
        logger.error(f"Clear cache error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# Autentik√°ci√≥s endpointok
class GenerateKeyRequest(BaseModel):
    name: str = Field(..., description="Kulcs neve")
    description: str = Field("", description="Kulcs le√≠r√°sa")


class RevokeKeyRequest(BaseModel):
    api_key_to_revoke: str = Field(..., description="Visszavonand√≥ API kulcs")


class VerifyKeyRequest(BaseModel):
    api_key_to_verify: str = Field(..., description="Ellen≈ërizend≈ë API kulcs")


@app.post("/api/auth/generate")
async def generate_api_key(request: GenerateKeyRequest):
    """API kulcs gener√°l√°sa (admin)"""
    try:
        api_key = api_key_manager.generate_key(request.name, request.description)
        return {
            "success": True,
            "api_key": api_key,  # Csak egyszer mutatjuk meg!
            "name": request.name,
            "warning": "Mentsd el ezt a kulcsot biztons√°gos helyre, mert nem fogod √∫jra l√°tni!"
        }
    except Exception as e:
        logger.error(f"Generate API key error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/auth/keys")
async def list_api_keys(api_key: Optional[str] = Security(verify_api_key)):
    """API kulcsok list√°z√°sa (n√©vvel √©s statisztik√°kkal)"""
    try:
        keys = api_key_manager.list_keys()
        return {
            "keys": keys,
            "count": len(keys)
        }
    except Exception as e:
        logger.error(f"List API keys error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/auth/revoke")
async def revoke_api_key(request: RevokeKeyRequest,
                        api_key: Optional[str] = Security(verify_api_key)):
    """API kulcs visszavon√°sa"""
    try:
        success = api_key_manager.revoke_key(request.api_key_to_revoke)
        if success:
            return {"success": True, "message": "API kulcs visszavonva"}
        else:
            raise HTTPException(status_code=404, detail="API kulcs nem tal√°lhat√≥")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Revoke API key error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/auth/verify")
async def verify_api_key_endpoint(request: VerifyKeyRequest):
    """API kulcs ellen≈ërz√©se (autentik√°ci√≥ n√©lk√ºl)"""
    try:
        is_valid = api_key_manager.validate_key(request.api_key_to_verify)
        return {
            "valid": is_valid,
            "message": "√ârv√©nyes API kulcs" if is_valid else "√ârv√©nytelen API kulcs"
        }
    except Exception as e:
        logger.error(f"Verify API key error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# GPU kezel√©si endpointok
@app.get("/api/gpu/status")
async def get_gpu_status(api_key: Optional[str] = Security(verify_api_key)):
    """GPU-k √°llapot√°nak lek√©r√©se"""
    try:
        gpus = gpu_manager.get_all_gpus_status()
        return {
            "gpus": gpus,
            "count": len(gpus),
            "available": len([g for g in gpus if g["status"] == "available"])
        }
    except Exception as e:
        logger.error(f"Get GPU status error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/gpu/available")
async def get_available_gpu(api_key: Optional[str] = Security(verify_api_key)):
    """El√©rhet≈ë GPU lek√©r√©se"""
    try:
        gpu_index = gpu_manager.get_available_gpu()
        if gpu_index is not None:
            gpu_info = gpu_manager.get_gpu_info(gpu_index)
            if gpu_info:
                return {
                    "gpu_index": gpu_index,
                    "name": gpu_info.name,
                    "memory_free": gpu_info.memory_total - gpu_info.memory_used,
                    "utilization": gpu_info.utilization
                }
        return {
            "gpu_index": None,
            "message": "Nincs el√©rhet≈ë GPU"
        }
    except Exception as e:
        logger.error(f"Get available GPU error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import sys
    # Reload csak fejleszt√©shez, √©les k√∂rnyezetben kikapcsolva
    # Kikapcsol√°s: python main.py --no-reload
    # Vagy k√∂rnyezeti v√°ltoz√≥val: export RELOAD=false
    use_reload = os.getenv("RELOAD", "false").lower() == "true" and "--no-reload" not in sys.argv
    
    # Network info ki√≠r√°sa szerver ind√≠t√°s el≈ëtt
    print_distributed_network_info()
    
    # Uvicorn konfigur√°ci√≥ reload warning elker√ºl√©s√©re
    if use_reload:
        uvicorn.run(
            "main:app",
            host="0.0.0.0",
            port=8000,
            reload=use_reload,
            reload_excludes=["*.pyc", "__pycache__", "*.log", "data/*", "logs/*", "projects/*", ".git/*"],
            reload_includes=["*.py"]  # Csak Python f√°jlok figyel√©se
        )
    else:
        # Production m√≥d: reload n√©lk√ºl, warning n√©lk√ºl
        uvicorn.run(
            "main:app",
            host="0.0.0.0",
            port=8000,
            reload=False
        )

